{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":793,"status":"ok","timestamp":1680757752907,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"5rOy3eDrv4z1"},"outputs":[],"source":["# # used a dictionary to represent an intents JSON file\n","# data = {\"intents\": [\n","# {\"tag\": \"greeting\",\n","#  \"responses\": [\"Howdy Partner!\", \"  \", \"How are you doing?\",   \"Greetings!\", \"How do you do?\"]},\n","# {\"tag\": \"age\",\n","#  \"responses\": [\"I am 25 years old\", \"I was born in 1998\", \"My birthday is July 3rd and I was born in 1998\", \"03/07/1998\"]},\n","# {\"tag\": \"date\",\n","#  \"responses\": [\"I am available all week\", \"I don't have any plans\",  \"I am not busy\"]},\n","# {\"tag\": \"name\",\n","#  \"responses\": [\"My name is James\", \"I'm James\", \"James\"]},\n","# {\"tag\": \"goodbye\",\n","#  \"responses\": [\"It was nice speaking to you\", \"See you later\", \"Speak soon!\"]}\n","# ]}"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4477,"status":"ok","timestamp":1680757757837,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"Sj2i2NaNv5wz","outputId":"5aec88e9-2894-4dcb-a9a2-d8030a1cb158"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13032,"status":"ok","timestamp":1680757770858,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"-t9K4gEDvxuR","outputId":"b4c44520-5408-477c-e734-de14f6c6beef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchinfo in /usr/local/lib/python3.9/dist-packages (1.7.2)\n"]}],"source":["# Install Transformers\n","!pip install transformers\n","# To get model summary\n","!pip install torchinfo"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3779,"status":"ok","timestamp":1680757774633,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"3nZF3IZtvxuW"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import torch\n","import random\n","import torch.nn as nn\n","import transformers\n","import matplotlib.pyplot as plt\n","# specify GPU\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1680757774640,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"oiROXSO7vxuX","outputId":"93d56c08-d0a3-49da-c644-5155364c639f"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-2ad2af76-810c-47f4-b79b-9a8c6a504524\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>how far is your Friday</td>\n","      <td>date</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>what do you want me to address you?</td>\n","      <td>name</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>how is your day going so far</td>\n","      <td>date</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>how is sunday going</td>\n","      <td>date</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I have sign off</td>\n","      <td>goodbye</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ad2af76-810c-47f4-b79b-9a8c6a504524')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2ad2af76-810c-47f4-b79b-9a8c6a504524 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2ad2af76-810c-47f4-b79b-9a8c6a504524');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                  text    label\n","0               how far is your Friday     date\n","1  what do you want me to address you?     name\n","2         how is your day going so far     date\n","3                  how is sunday going     date\n","4                      I have sign off  goodbye"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# We have prepared a chitchat dataset with 5 labels\n","df = pd.read_excel(\"/content/drive/MyDrive/Chatbot using BERT and Pytorch/dataset.xlsx\")\n","df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1680757774640,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"FSaJRss7wk4w","outputId":"3da6f9ed-486b-4cb2-95c6-38f22a73a34e"},"outputs":[{"data":{"text/plain":["greeting    5\n","name        4\n","goodbye     4\n","date        3\n","age         3\n","Name: label, dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df['label'].value_counts()"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1693,"status":"ok","timestamp":1680757776318,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"ud1aoNoWwnJs","outputId":"2f5561b0-cd17-4f03-9ced-9a4109940f14"},"outputs":[{"data":{"text/plain":["3    0.263158\n","4    0.210526\n","2    0.210526\n","1    0.157895\n","0    0.157895\n","Name: label, dtype: float64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Converting the labels into encodings\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","df['label'] = le.fit_transform(df['label'])\n","# check class distribution\n","df['label'].value_counts(normalize = True)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680757776318,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"PbnTpH6swwMi"},"outputs":[],"source":["# In this example we have used all the utterances for training purpose\n","train_text, train_labels = df['text'], df['label']"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680757776318,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"JEzHOt6AxxWh","outputId":"ab25afdc-9973-4c1a-822d-fe27c7d504c0"},"outputs":[{"data":{"text/plain":["0     1\n","1     4\n","2     1\n","3     1\n","4     2\n","5     3\n","6     3\n","7     3\n","8     3\n","9     3\n","10    0\n","11    0\n","12    0\n","13    4\n","14    4\n","15    4\n","16    2\n","17    2\n","18    2\n","Name: label, dtype: int64"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["\n","train_labels"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1089,"status":"ok","timestamp":1680757777402,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"DR-Crp-mxyj8","outputId":"abf21f7e-ccc9-466b-dbf0-03e7216fcda2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import AutoModel, BertTokenizerFast\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","# Import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2313,"status":"ok","timestamp":1680757779709,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"6HmNdQ4ZyPwX","outputId":"a1be8c34-7187-4c28-f927-82c7cd3de6aa"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import RobertaTokenizer, RobertaModel\n","# Load the Roberta tokenizer\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","# Import Roberta pretrained model\n","bert = RobertaModel.from_pretrained('roberta-base')"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":773,"status":"ok","timestamp":1680757780479,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"ejO6KQSKyWNv","outputId":"c4aa9ca3-d1e4-4127-a13b-7c7805a7a8eb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import DistilBertTokenizer, DistilBertModel\n","# Load the DistilBert tokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","# Import the DistilBert pretrained model\n","bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680757780479,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"e0mOk4-6ya8i","outputId":"b8166c02-70fb-480c-f04b-ce35a3de4871"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[  101,  2023,  2003,  1037,  4487, 16643,  2140, 14324,  2944,  1012,\n","           102],\n","        [  101,  2951,  2003,  3514,   102,     0,     0,     0,     0,     0,\n","             0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"]}],"source":["text = [\"this is a distil bert model.\",\"data is oil\"]\n","# Encode the text\n","encoded_input = tokenizer(text, padding=True,truncation=True, return_tensors='pt')\n","print(encoded_input)\n","\n","# In input_ids:\n","# 101 - Indicates beginning of the sentence\n","# 102 - Indicates end of the sentence\n","# In attention_mask:\n","# 1 - Actual token\n","# 0 - Padded token"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"executionInfo":{"elapsed":891,"status":"ok","timestamp":1680757781365,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"NX4nJftQyirc","outputId":"f59e8a39-2744-4f15-8bf9-f551d03685a8"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# get length of all the messages in the train set\n","seq_len = [len(i.split()) for i in train_text]\n","pd.Series(seq_len).hist(bins = 10)\n","# Based on the histogram we are selecting the max len as 8\n","max_seq_len = 8"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1680757781366,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"gFMBmeUoywd6","outputId":"b6609df5-36ef-4a9a-9105-d0b567f909ee"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["# tokenize and encode sequences in the training set\n","tokens_train = tokenizer(\n","    train_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1680757781366,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"NoUlowmry00Z"},"outputs":[],"source":["# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1680757781366,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"zbAJk7h-y5ZT"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","#define a batch size\n","batch_size = 16\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","# DataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1680757781367,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"UTCFqNTU0o3O"},"outputs":[],"source":["class BERT_Arch(nn.Module):\n","   def __init__(self, bert):      \n","       super(BERT_Arch, self).__init__()\n","       self.bert = bert \n","      \n","       # dropout layer\n","       self.dropout = nn.Dropout(0.2)\n","      \n","       # relu activation function\n","       self.relu =  nn.ReLU()\n","       # dense layer\n","       self.fc1 = nn.Linear(768,512)\n","       self.fc2 = nn.Linear(512,256)\n","       self.fc3 = nn.Linear(256,5)\n","       #softmax activation function\n","       self.softmax = nn.LogSoftmax(dim=1)\n","       #define the forward pass\n","   def forward(self, sent_id, mask):\n","      #pass the inputs to the model  \n","      cls_hs = self.bert(sent_id, attention_mask=mask)[0][:,0]\n","      \n","      x = self.fc1(cls_hs)\n","      x = self.relu(x)\n","      x = self.dropout(x)\n","      \n","      x = self.fc2(x)\n","      x = self.relu(x)\n","      x = self.dropout(x)\n","      # output layer\n","      x = self.fc3(x)\n","   \n","      # apply softmax activation\n","      x = self.softmax(x)\n","      return x"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1680757781367,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"_uj-yPA70rD1","outputId":"b60b1f1c-7b43-4ccd-8dc0-3721a2fc12f0"},"outputs":[{"data":{"text/plain":["================================================================================\n","Layer (type:depth-idx)                                  Param #\n","================================================================================\n","BERT_Arch                                               --\n","├─DistilBertModel: 1-1                                  --\n","│    └─Embeddings: 2-1                                  --\n","│    │    └─Embedding: 3-1                              (23,440,896)\n","│    │    └─Embedding: 3-2                              (393,216)\n","│    │    └─LayerNorm: 3-3                              (1,536)\n","│    │    └─Dropout: 3-4                                --\n","│    └─Transformer: 2-2                                 --\n","│    │    └─ModuleList: 3-5                             (42,527,232)\n","├─Dropout: 1-2                                          --\n","├─ReLU: 1-3                                             --\n","├─Linear: 1-4                                           393,728\n","├─Linear: 1-5                                           131,328\n","├─Linear: 1-6                                           1,285\n","├─LogSoftmax: 1-7                                       --\n","================================================================================\n","Total params: 66,889,221\n","Trainable params: 526,341\n","Non-trainable params: 66,362,880\n","================================================================================"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# freeze all the parameters. This will prevent updating of model weights during fine-tuning.\n","for param in bert.parameters():\n","      param.requires_grad = False\n","model = BERT_Arch(bert)\n","# push the model to GPU\n","model = model.to(device)\n","from torchinfo import summary\n","summary(model)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2970,"status":"ok","timestamp":1680757784321,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"3qDPDGp80u3J","outputId":"e84ae167-318f-4dc8-8473-24e313705082"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["from transformers import AdamW\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 1e-3)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1680757784322,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"PnezLtWz030M","outputId":"e458943b-08cb-4728-e362-4fd75a6f95be"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.26666667 1.26666667 0.95       0.76       0.95      ]\n"]}],"source":["from sklearn.utils.class_weight import compute_class_weight\n","#compute the class weights\n","class_wts = compute_class_weight(\n","                                        class_weight = \"balanced\",\n","                                        classes = np.unique(train_labels),\n","                                        y = train_labels                                                    \n","                                    )\n","print(class_wts)"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680757784322,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"Z1BHyrCY06eq"},"outputs":[],"source":["# convert class weights to tensor\n","weights= torch.tensor(class_wts,dtype=torch.float)\n","weights = weights.to(device)\n","# loss function\n","cross_entropy = nn.NLLLoss(weight=weights)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1680757784323,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"FZBtO5hW1hay"},"outputs":[],"source":["# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","# number of training epochs\n","epochs = 200\n","# We can also use learning rate scheduler to achieve better results\n","lr_sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1680757784323,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"h84MxZTZ1p7j"},"outputs":[],"source":["# function to train the model\n","total_loss = 0.0123\n","def train():\n","  \n","  model.train()\n","  total_loss = 0\n","  \n","  # empty list to save model predictions\n","  total_preds=[]\n","  \n","  # iterate over batches\n","  for step,batch in enumerate(train_dataloader):\n","    \n","    # progress update after every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step,    len(train_dataloader)))\n","    # push the batch to gpu\n","    batch = [r.to(device) for r in batch] \n","    sent_id, mask, labels = batch\n","    # get model predictions for the current batch\n","    preds = model(sent_id, mask)\n","    # compute the loss between actual and predicted values\n","    loss = cross_entropy(preds, labels)\n","    # add on to the total loss\n","    total_loss = total_loss + loss.item()\n","    # backward pass to calculate the gradients\n","    loss.backward()\n","    # clip the the gradients to 1.0. It helps in preventing the    exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","    # update parameters\n","    optimizer.step()\n","    # clear calculated gradients\n","    optimizer.zero_grad()\n","  \n","    # We are not using learning rate scheduler as of now\n","    # lr_sch.step()\n","    # model predictions are stored on GPU. So, push it to CPU\n","    preds=preds.detach().cpu().numpy()\n","    # append the model predictions\n","    total_preds.append(preds)\n","# compute the training loss of the epoch\n","  avg_loss = total_loss / len(train_dataloader)\n","    \n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","#returns the loss and predictions\n","  return avg_loss, total_preds"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7649,"status":"ok","timestamp":1680757791960,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"TV6WjKKM2_mE","outputId":"19c67b13-7d5a-4cd2-985d-cf982d2b288c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Epoch 1 / 200\n","\n"," Epoch 2 / 200\n","\n"," Epoch 3 / 200\n","\n"," Epoch 4 / 200\n","\n"," Epoch 5 / 200\n","\n"," Epoch 6 / 200\n","\n"," Epoch 7 / 200\n","\n"," Epoch 8 / 200\n","\n"," Epoch 9 / 200\n","\n"," Epoch 10 / 200\n","\n"," Epoch 11 / 200\n","\n"," Epoch 12 / 200\n","\n"," Epoch 13 / 200\n","\n"," Epoch 14 / 200\n","\n"," Epoch 15 / 200\n","\n"," Epoch 16 / 200\n","\n"," Epoch 17 / 200\n","\n"," Epoch 18 / 200\n","\n"," Epoch 19 / 200\n","\n"," Epoch 20 / 200\n","\n"," Epoch 21 / 200\n","\n"," Epoch 22 / 200\n","\n"," Epoch 23 / 200\n","\n"," Epoch 24 / 200\n","\n"," Epoch 25 / 200\n","\n"," Epoch 26 / 200\n","\n"," Epoch 27 / 200\n","\n"," Epoch 28 / 200\n","\n"," Epoch 29 / 200\n","\n"," Epoch 30 / 200\n","\n"," Epoch 31 / 200\n","\n"," Epoch 32 / 200\n","\n"," Epoch 33 / 200\n","\n"," Epoch 34 / 200\n","\n"," Epoch 35 / 200\n","\n"," Epoch 36 / 200\n","\n"," Epoch 37 / 200\n","\n"," Epoch 38 / 200\n","\n"," Epoch 39 / 200\n","\n"," Epoch 40 / 200\n","\n"," Epoch 41 / 200\n","\n"," Epoch 42 / 200\n","\n"," Epoch 43 / 200\n","\n"," Epoch 44 / 200\n","\n"," Epoch 45 / 200\n","\n"," Epoch 46 / 200\n","\n"," Epoch 47 / 200\n","\n"," Epoch 48 / 200\n","\n"," Epoch 49 / 200\n","\n"," Epoch 50 / 200\n","\n"," Epoch 51 / 200\n","\n"," Epoch 52 / 200\n","\n"," Epoch 53 / 200\n","\n"," Epoch 54 / 200\n","\n"," Epoch 55 / 200\n","\n"," Epoch 56 / 200\n","\n"," Epoch 57 / 200\n","\n"," Epoch 58 / 200\n","\n"," Epoch 59 / 200\n","\n"," Epoch 60 / 200\n","\n"," Epoch 61 / 200\n","\n"," Epoch 62 / 200\n","\n"," Epoch 63 / 200\n","\n"," Epoch 64 / 200\n","\n"," Epoch 65 / 200\n","\n"," Epoch 66 / 200\n","\n"," Epoch 67 / 200\n","\n"," Epoch 68 / 200\n","\n"," Epoch 69 / 200\n","\n"," Epoch 70 / 200\n","\n"," Epoch 71 / 200\n","\n"," Epoch 72 / 200\n","\n"," Epoch 73 / 200\n","\n"," Epoch 74 / 200\n","\n"," Epoch 75 / 200\n","\n"," Epoch 76 / 200\n","\n"," Epoch 77 / 200\n","\n"," Epoch 78 / 200\n","\n"," Epoch 79 / 200\n","\n"," Epoch 80 / 200\n","\n"," Epoch 81 / 200\n","\n"," Epoch 82 / 200\n","\n"," Epoch 83 / 200\n","\n"," Epoch 84 / 200\n","\n"," Epoch 85 / 200\n","\n"," Epoch 86 / 200\n","\n"," Epoch 87 / 200\n","\n"," Epoch 88 / 200\n","\n"," Epoch 89 / 200\n","\n"," Epoch 90 / 200\n","\n"," Epoch 91 / 200\n","\n"," Epoch 92 / 200\n","\n"," Epoch 93 / 200\n","\n"," Epoch 94 / 200\n","\n"," Epoch 95 / 200\n","\n"," Epoch 96 / 200\n","\n"," Epoch 97 / 200\n","\n"," Epoch 98 / 200\n","\n"," Epoch 99 / 200\n","\n"," Epoch 100 / 200\n","\n"," Epoch 101 / 200\n","\n"," Epoch 102 / 200\n","\n"," Epoch 103 / 200\n","\n"," Epoch 104 / 200\n","\n"," Epoch 105 / 200\n","\n"," Epoch 106 / 200\n","\n"," Epoch 107 / 200\n","\n"," Epoch 108 / 200\n","\n"," Epoch 109 / 200\n","\n"," Epoch 110 / 200\n","\n"," Epoch 111 / 200\n","\n"," Epoch 112 / 200\n","\n"," Epoch 113 / 200\n","\n"," Epoch 114 / 200\n","\n"," Epoch 115 / 200\n","\n"," Epoch 116 / 200\n","\n"," Epoch 117 / 200\n","\n"," Epoch 118 / 200\n","\n"," Epoch 119 / 200\n","\n"," Epoch 120 / 200\n","\n"," Epoch 121 / 200\n","\n"," Epoch 122 / 200\n","\n"," Epoch 123 / 200\n","\n"," Epoch 124 / 200\n","\n"," Epoch 125 / 200\n","\n"," Epoch 126 / 200\n","\n"," Epoch 127 / 200\n","\n"," Epoch 128 / 200\n","\n"," Epoch 129 / 200\n","\n"," Epoch 130 / 200\n","\n"," Epoch 131 / 200\n","\n"," Epoch 132 / 200\n","\n"," Epoch 133 / 200\n","\n"," Epoch 134 / 200\n","\n"," Epoch 135 / 200\n","\n"," Epoch 136 / 200\n","\n"," Epoch 137 / 200\n","\n"," Epoch 138 / 200\n","\n"," Epoch 139 / 200\n","\n"," Epoch 140 / 200\n","\n"," Epoch 141 / 200\n","\n"," Epoch 142 / 200\n","\n"," Epoch 143 / 200\n","\n"," Epoch 144 / 200\n","\n"," Epoch 145 / 200\n","\n"," Epoch 146 / 200\n","\n"," Epoch 147 / 200\n","\n"," Epoch 148 / 200\n","\n"," Epoch 149 / 200\n","\n"," Epoch 150 / 200\n","\n"," Epoch 151 / 200\n","\n"," Epoch 152 / 200\n","\n"," Epoch 153 / 200\n","\n"," Epoch 154 / 200\n","\n"," Epoch 155 / 200\n","\n"," Epoch 156 / 200\n","\n"," Epoch 157 / 200\n","\n"," Epoch 158 / 200\n","\n"," Epoch 159 / 200\n","\n"," Epoch 160 / 200\n","\n"," Epoch 161 / 200\n","\n"," Epoch 162 / 200\n","\n"," Epoch 163 / 200\n","\n"," Epoch 164 / 200\n","\n"," Epoch 165 / 200\n","\n"," Epoch 166 / 200\n","\n"," Epoch 167 / 200\n","\n"," Epoch 168 / 200\n","\n"," Epoch 169 / 200\n","\n"," Epoch 170 / 200\n","\n"," Epoch 171 / 200\n","\n"," Epoch 172 / 200\n","\n"," Epoch 173 / 200\n","\n"," Epoch 174 / 200\n","\n"," Epoch 175 / 200\n","\n"," Epoch 176 / 200\n","\n"," Epoch 177 / 200\n","\n"," Epoch 178 / 200\n","\n"," Epoch 179 / 200\n","\n"," Epoch 180 / 200\n","\n"," Epoch 181 / 200\n","\n"," Epoch 182 / 200\n","\n"," Epoch 183 / 200\n","\n"," Epoch 184 / 200\n","\n"," Epoch 185 / 200\n","\n"," Epoch 186 / 200\n","\n"," Epoch 187 / 200\n","\n"," Epoch 188 / 200\n","\n"," Epoch 189 / 200\n","\n"," Epoch 190 / 200\n","\n"," Epoch 191 / 200\n","\n"," Epoch 192 / 200\n","\n"," Epoch 193 / 200\n","\n"," Epoch 194 / 200\n","\n"," Epoch 195 / 200\n","\n"," Epoch 196 / 200\n","\n"," Epoch 197 / 200\n","\n"," Epoch 198 / 200\n","\n"," Epoch 199 / 200\n","\n"," Epoch 200 / 200\n","\n","Training Loss: 0.002\n"]}],"source":["# start the\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss, _ = train()\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    # it can make your experiment reproducible, similar to set  random seed to all options where there needs a random seed.\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","print(f'\\nTraining Loss: {train_loss:.3f}')"]},{"cell_type":"markdown","metadata":{"id":"UT48hHN14bXz"},"source":["### Get Predictions for Test Data"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1680757791961,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"DamY6BAD4clG"},"outputs":[],"source":["def get_prediction(str):\n"," str = re.sub(r'[^a-zA-Z ]+', '', str)\n"," test_text = [str]\n"," model.eval()\n"," \n"," tokens_test_data = tokenizer(\n"," test_text,\n"," max_length = max_seq_len,\n"," pad_to_max_length=True,\n"," truncation=True,\n"," return_token_type_ids=False\n"," )\n"," test_seq = torch.tensor(tokens_test_data['input_ids'])\n"," test_mask = torch.tensor(tokens_test_data['attention_mask'])\n"," \n"," preds = None\n"," with torch.no_grad():\n","   preds = model(test_seq.to(device), test_mask.to(device))\n"," preds = preds.detach().cpu().numpy()\n"," preds = np.argmax(preds, axis = 1)\n"," print('Intent Identified: ', le.inverse_transform(preds)[0])\n"," return le.inverse_transform(preds)[0]\n","def get_response(message): \n","  intent = get_prediction(message)\n","  for i in data['intents']: \n","    if i[\"tag\"] == intent:\n","      result = random.choice(i[\"responses\"])\n","      break\n","  print(f\"Response : {result}\")\n","  return \"Intent: \"+ intent + '\\n' + \"Response: \" + result"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1680757791961,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"MjTv8cta4e3Z","outputId":"b5cd2a1f-c2a1-4f7b-922f-7e42ec639231"},"outputs":[{"name":"stdout","output_type":"stream","text":["Intent Identified:  name\n","Response : James\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Intent: name\\nResponse: James'"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["get_response('who are you' )"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1680757791961,"user":{"displayName":"Sandesh Hase","userId":"13156194362961116144"},"user_tz":-330},"id":"vgVTZ85Y8r1R"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"sample","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
